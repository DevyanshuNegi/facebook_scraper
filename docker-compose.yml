version: '3.8'

# Root-level docker-compose.yml for convenience
# Actual configuration is in docker/docker-compose.yml

services:
  scraper-api:
    build:
      context: .
      dockerfile: docker/Dockerfile
    container_name: facebook-scraper-api
    ports:
      - "3000:3000"
    environment:
      - PORT=${PORT:-3000}
      - POLLING_INTERVAL_MS=${POLLING_INTERVAL_MS:-12000}
      - PARALLEL_BATCH_SIZE=${PARALLEL_BATCH_SIZE:-5}
      - GOOGLE_SERVICE_ACCOUNT_EMAIL=${GOOGLE_SERVICE_ACCOUNT_EMAIL}
      - GOOGLE_PRIVATE_KEY=${GOOGLE_PRIVATE_KEY}
      - FACEBOOK_COOKIES=${FACEBOOK_COOKIES}
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "node", "-e", "require('http').get('http://localhost:3000/health', (r) => {process.exit(r.statusCode === 200 ? 0 : 1)})" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    volumes:
      # Mount results directory to persist data (optional)
      - ./data/results:/app/results
    networks:
      - scraper-network
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

networks:
  scraper-network:
    driver: bridge
